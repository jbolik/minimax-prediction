{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.numpy.linalg import slogdet\n",
    "from jax.scipy.stats import multivariate_normal as mvtn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from t import t_logpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X1: jnp.ndarray,\n",
    "                X2: jnp.ndarray,\n",
    "                lengthscale: float) -> jnp.ndarray:\n",
    "    sqdist = jnp.sum((X1[:, None, :] - X2[None, :, :]) ** 2, axis=-1)\n",
    "    return jnp.exp(-0.5 * sqdist / (lengthscale ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP model\n",
    "\n",
    "def log_pdf(y, X, lengthscale, amplitude, beta):\n",
    "    return mvtn.logpdf(y, X @ beta, amplitude**2 * rbf_kernel(X, X, lengthscale))\n",
    "\n",
    "def sample(key, X, lengthscale, amplitude, beta):\n",
    "    return jax.random.multivariate_normal(key, X @ beta, amplitude**2 * rbf_kernel(X, X, lengthscale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are working with a fixed lengthscale\n",
    "lengthscale = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.numpy.linalg import inv\n",
    "\n",
    "# dof=n-p for right-invariant prior and dof=n for Jeffreys prior\n",
    "def log_pred(yp, Xp, yo, Xo, dof):\n",
    "    n_obs = len(yo)\n",
    "    y = jnp.vstack((yo, yp))\n",
    "    Xo = jnp.vstack((Xo, Xp))\n",
    "    K = rbf_kernel(Xo, Xo, lengthscale)\n",
    "    K_inv = inv(K)\n",
    "    A = K_inv - K_inv @ Xo @ inv(Xo.T @ K_inv @ Xo) @ Xo.T @ K_inv\n",
    "    Aoo, Aop, Apo, App = A[:n_obs, :n_obs], A[:n_obs, n_obs:], A[n_obs:, :n_obs], A[n_obs:, n_obs:]\n",
    "    App_inv = inv(App)\n",
    "    Sigma = (yo.T @ (Aoo - Aop @ App_inv @ Apo) @ yo / dof) * App_inv\n",
    "    mu = - App_inv @ Apo @ yo\n",
    "    return t_logpdf(yp, mu, Sigma, dof)\n",
    "\n",
    "# dof=n-p for unbiased and dof=n for MLE\n",
    "def log_pred_plug_in(yp, Xp, yo, Xo, dof):\n",
    "    K = rbf_kernel(Xo, Xo, lengthscale)\n",
    "    K_inv = inv(K)\n",
    "    beta_hat = inv(Xo.T @ K_inv @ Xo) @ Xo.T @ K_inv @ yo\n",
    "    amp_hat = jnp.sqrt((yo - Xo @ beta_hat).T @ K_inv @ (yo - Xo @ beta_hat) / dof)\n",
    "    return log_pdf(yp, Xp, lengthscale, amp_hat, beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.203939   0.99705505 1.5386127  1.7187254 ] [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# numerically evaluate predictive procedures against knowing true parameters\n",
    "n, p = 4, 2\n",
    "@jax.jit\n",
    "def mc_estimate_risk(key, Xo, Xp, amplitude, beta):\n",
    "    samples = jax.vmap(sample, (0, None, None, None, None))(jax.random.split(key, n+1), Xp, lengthscale, amplitude, beta)\n",
    "    yo, yp = samples[:n], samples[n:]\n",
    "    true_pdf = log_pdf(yp, Xp, lengthscale, amplitude, beta)\n",
    "    return jnp.array([\n",
    "        true_pdf - log_pred(yp, Xp, yo, Xo, dof=n-p),\n",
    "        true_pdf - log_pred(yp, Xp, yo, Xo, dof=n),\n",
    "        true_pdf - log_pred_plug_in(yp, Xp, yo, Xo, dof=n-p),\n",
    "        true_pdf - log_pred_plug_in(yp, Xp, yo, Xo, dof=n)])\n",
    "\n",
    "samples = 8192 * 4\n",
    "Xo = jnp.array([[0, 0], [-10, 0], [0, 1], [2, 3]], float)\n",
    "Xp = jnp.array([[1, 1]], float)\n",
    "iters = 2 ** 12\n",
    "key = jax.random.PRNGKey(0)\n",
    "scores, nans = jnp.zeros((4,)), jnp.zeros((4,))\n",
    "for i in range(iters):\n",
    "    key_now, key = jax.random.split(key, 2)\n",
    "    keys = jax.random.split(key_now, samples)\n",
    "    results = jax.vmap(mc_estimate_risk, (0, None, None, None, None))(keys, Xo, Xp, 3.0, jnp.array([4.0, 15.0]))[:, :, 0]\n",
    "    scores += jnp.nanmean(results, axis=0)\n",
    "    nans += jnp.mean(jnp.isnan(results), axis=0)\n",
    "print(scores / iters, nans / iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive risk doesn't seem to be invariant in the parameters\n",
    "For amp=3 beta=[4, -5]: [ 0.42137426  0.7708848   6.611105   13.481195  ] [2.3841858e-07 2.3841858e-07 0.0000000e+00 0.0000000e+00]\n",
    "For amp=3 beta=[4, 15]: [1.203939   0.99705505 1.5386127  1.7187254 ] [0. 0. 0. 0.]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
